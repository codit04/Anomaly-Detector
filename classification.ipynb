{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Initially thought of estimating forecastability using Approximate Entropy,Sample Entropy and the Average Demand Interval(ADI),square of coefficient of variation(CV^2) methods but the hint provided directed me towards using the features of the Time Series.\n",
    "\n",
    " And since, the Prophet model has to be used for anomaly detection, I generated synthetic data for the time series using the Prophet model and then decided to extract the features from the generated data and compare it with the provided data for calculating the forecastability score.\n",
    "\n",
    "Feature Vector Generation :  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c8d66dd1cdf04d6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nolds\n",
    "from scipy.stats import skew, kurtosis, entropy,variation\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf,kpss\n",
    "from arch.unitroot import PhillipsPerron\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def featureVector(json):\n",
    "    df= pd.read_json(json)\n",
    "    df['point_timestamp'] = pd.to_datetime(df['point_timestamp'])\n",
    "    df['point_value'] = pd.to_numeric(df['point_value'])\n",
    "    \n",
    "    #Features\n",
    "    feature_vector = {}\n",
    "    feature_vector['SampEn'] = nolds.sampen(df['point_value']) #Sample Entropy\n",
    "    feature_vector['Hurst'] = nolds.hurst_rs(df['point_value']) #Hurst Exponent\n",
    "    feature_vector['dfa'] = nolds.dfa(df['point_value']) #Detrended Fluctuation Analysis\n",
    "    feature_vector['Skew'] = skew(df['point_value']) #Skewness\n",
    "    feature_vector['Kurt'] = kurtosis(df['point_value']) #Kurtosis\n",
    "    feature_vector['Entropy'] = entropy(df['point_value']) #Entropy\n",
    "    time_intervals = df['point_value'].diff().dropna()\n",
    "    feature_vector['ADI'] = time_intervals.mean() #Average Demand Interval\n",
    "    cv=df['point_value'].std()/df['point_value'].mean()\n",
    "    feature_vector['cv2'] = cv**2   # Coefficient of Variation\n",
    "    feature_vector['ADF'] = adfuller(df['point_value'])[1] #Augmented Dickey-Fuller Test\n",
    "    feature_vector['ACF'] = acf(df['point_value'])[1] #Auto Correlation\n",
    "    feature_vector['PACF'] = pacf(df['point_value'])[1] #Partial Auto Correlation\n",
    "    feature_vector['KPSS'] = kpss(df['point_value'])[1] #Kwiatkowski-Phillips-Schmidt-Shin Test\n",
    "    feature_vector['Stationary'] = bool(feature_vector['ADF'] <= 0.05 and feature_vector['KPSS'] > 0.05) #Stationarity Check\n",
    "    feature_vector['PP'] = PhillipsPerron(df['point_value']).pvalue#Phillips-Perron Test\n",
    "    feature_vector['Normality'] = bool(sm.stats.diagnostic.normal_ad(df['point_value'])[1] > 0.05) #Normality Check\n",
    "    decompose = sm.tsa.seasonal_decompose(df['point_value'], model='additive', period=6) #Seasonal Decomposition -Half yearly since few time series given have less records \n",
    "    trend = decompose.trend.dropna()\n",
    "    seasonal = decompose.seasonal.dropna()\n",
    "    residual = decompose.resid.dropna()\n",
    "    seasonal = decompose.seasonal.dropna()\n",
    "    feature_vector['Trend Strenth'] = max(0,variation(residual)/(variation(trend)+variation(residual))) #Trend Strength\n",
    "    feature_vector['Seasonal Strength'] = max(0,variation(residual)/(variation(seasonal)+variation(residual))) #Seasonal Strength\n",
    "    rolling_mean = df['point_value'].rolling(window=7).mean()\n",
    "    rolling_std = df['point_value'].rolling(window=7).std()\n",
    "    feature_vector['Rolling Mean'] = rolling_mean.mean() #Rolling Mean\n",
    "    feature_vector['Rolling Std'] = rolling_std.mean() #Rolling Standard Deviation\n",
    "    for key in feature_vector.keys():\n",
    "        if feature_vector[key] == float('inf') or feature_vector[key] == float('-inf') or feature_vector[key] == float('nan'):\n",
    "            feature_vector[key] = 0\n",
    "    return feature_vector\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T10:40:53.740761Z",
     "start_time": "2024-03-27T10:40:20.735531Z"
    }
   },
   "id": "b498540e6cde9f14",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def forecastability_score_euclidean(feature_vector1,feature_vector2):\n",
    "    # performing euclidean distance between the two feature vectors and return the score in the range [0,10]\n",
    "    score = 0\n",
    "    for key in feature_vector1.keys():\n",
    "        score += (feature_vector1[key]-feature_vector2[key])**2\n",
    "        print(key,feature_vector1[key],feature_vector2[key])\n",
    "    return 10/(1+score)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T11:01:42.497661Z",
     "start_time": "2024-03-24T11:01:41.855014Z"
    }
   },
   "id": "f060e2af17ed3d62",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def forecastability_score_cosine(feature_vector1, feature_vector2):\n",
    "    # find cosine similarity between 2 lists given as parameters\n",
    "    #reshape to 2d\n",
    "    feature_vector1 = np.array(feature_vector1).reshape(1,-1)\n",
    "    feature_vector2 = np.array(feature_vector2).reshape(1,-1)\n",
    "    similarity = cosine_similarity(feature_vector1,feature_vector2)[0][0]\n",
    "    similarity = 5*(similarity+1)\n",
    "    return similarity\n",
    "\n",
    "def forcastability_score_cosine_with_rep(feature_vector):\n",
    "    representative=pd.read_json('/home/codit/PycharmProjects/DataGenie-Hackathon/Prophet Data/representative.json',typ='series')\n",
    "    representative=pd.DataFrame(representative)\n",
    "    rep=representative.iloc[:,0].tolist()\n",
    "    rep = np.array(rep).reshape(1,-1)\n",
    "    feature_vector = np.array(feature_vector).reshape(1,-1)\n",
    "    similarity = cosine_similarity(rep,feature_vector)[0][0]\n",
    "    similarity = 5*(similarity+1)\n",
    "    return similarity"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T10:48:51.157696Z",
     "start_time": "2024-03-27T10:48:45.770042Z"
    }
   },
   "id": "1db5a1574882c204",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def findrepresentative():\n",
    "    df=[]\n",
    "    for i in range(5000):\n",
    "        f=featureVector('/home/codit/PycharmProjects/DataGenie-Hackathon/Prophet Data/sample_'+str(i)+'.json')\n",
    "        df.append(f)\n",
    "    df=pd.DataFrame(df)\n",
    "    return df.mean()   # making the mean to be the representative feature vector for the synthetic Prophet data\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T11:01:50.805759Z",
     "start_time": "2024-03-24T11:01:50.340883Z"
    }
   },
   "id": "df704a45312b6b1d",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "representative=findrepresentative()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54be9f36cf32c187",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Added the representative found to the representative.json file in the prophet data repository.\n",
    "Now towards finding the threshold for the forecastability score. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2819d3ded6cca5a1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'representative' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mrepresentative\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'representative' is not defined"
     ]
    }
   ],
   "source": [
    "representative"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T10:51:19.239285Z",
     "start_time": "2024-03-27T10:51:19.190818Z"
    }
   },
   "id": "13a056996f9eab4a",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'representative' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mrepresentative\u001B[49m\u001B[38;5;241m.\u001B[39mto_json(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/home/codit/PycharmProjects/DataGenie-Hackathon/Prophet Data/representative.json\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'representative' is not defined"
     ]
    }
   ],
   "source": [
    "representative.to_json('/home/codit/PycharmProjects/DataGenie-Hackathon/Prophet Data/representative.json')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T10:51:12.616571Z",
     "start_time": "2024-03-27T10:51:12.512566Z"
    }
   },
   "id": "314415ad0602c028",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "representative=pd.read_json('/home/codit/PycharmProjects/DataGenie-Hackathon/Prophet Data/representative.json',typ='series')\n",
    "representative=pd.DataFrame(representative)\n",
    "rep=representative.iloc[:,0].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T10:51:24.055727Z",
     "start_time": "2024-03-27T10:51:23.349550Z"
    }
   },
   "id": "aa92a8ffed044380",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "forcastabilility score for the sample time series data    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17f2f795b047843f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "def findForcastabilityScore():\n",
    "    scores=[]\n",
    "    representative=rep\n",
    "    hourly = os.listdir(\"/home/codit/Documents/Time Series JSON/hourly\")\n",
    "    daily = os.listdir(\"/home/codit/Documents/Time Series JSON/daily\")\n",
    "    weekly = os.listdir(\"/home/codit/Documents/Time Series JSON/weekly\")\n",
    "    monthly = os.listdir(\"/home/codit/Documents/Time Series JSON/monthly\")\n",
    "    for i in hourly:\n",
    "        f=list(featureVector(\"/home/codit/Documents/Time Series JSON/hourly/\"+i).values())\n",
    "        scores.append(forecastability_score_cosine(representative,f))\n",
    "    for i in daily:\n",
    "        f=list(featureVector(\"/home/codit/Documents/Time Series JSON/daily/\"+i).values())\n",
    "        scores.append(forecastability_score_cosine(representative,f))\n",
    "    for i in weekly:\n",
    "        f=list(featureVector(\"/home/codit/Documents/Time Series JSON/weekly/\"+i).values())\n",
    "        scores.append(forecastability_score_cosine(representative,f))\n",
    "    for i in monthly:\n",
    "        f=list(featureVector(\"/home/codit/Documents/Time Series JSON/monthly/\"+i).values())\n",
    "        scores.append(forecastability_score_cosine(representative,f))\n",
    "    return scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T10:51:35.298436Z",
     "start_time": "2024-03-27T10:51:35.285246Z"
    }
   },
   "id": "1683aac29c597253",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sampleforecastScores=findForcastabilityScore()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd55230345c5b29f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Approach to calculate threshold is to iterate over the score with a step value and find the score that provides with least RMSE with the provided forecastability score."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f516ba3cc348e4aa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import math\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import root_mean_squared_error,mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "def forecastabilityThreshold(sampleforecastScores):\n",
    "    sampleforecastScores=pd.Series(sampleforecastScores)\n",
    "    step = sampleforecastScores.diff().abs().mean()\n",
    "    start = math.floor(sampleforecastScores.median())\n",
    "    end = math.ceil(sampleforecastScores.max())\n",
    "    thresholdRange = np.arange(start,end,step)\n",
    "    threshold=0\n",
    "    minMAPEGen = 100000000000\n",
    "    visited={}\n",
    "    for i in thresholdRange:\n",
    "        rmse=[]\n",
    "        hourly = os.listdir(\"/home/codit/Documents/Time Series JSON/hourly\")\n",
    "        daily = os.listdir(\"/home/codit/Documents/Time Series JSON/daily\")\n",
    "        weekly = os.listdir(\"/home/codit/Documents/Time Series JSON/weekly\")\n",
    "        monthly = os.listdir(\"/home/codit/Documents/Time Series JSON/monthly\")\n",
    "        \n",
    "        for j in hourly:\n",
    "            if \"hourly\"+j in list(visited.keys()) and visited[\"hourly\"+j]>=i:\n",
    "                rmse.append(visited[\"hourly\"+j])\n",
    "            else:\n",
    "                f=list(featureVector(\"/home/codit/Documents/Time Series JSON/hourly/\"+j).values())\n",
    "                if forcastability_score_cosine_with_rep(f)>=i:\n",
    "                    visited[\"hourly\"+j]=predict(pd.read_json(\"/home/codit/Documents/Time Series JSON/hourly/\"+j))\n",
    "                    rmse.append(visited[\"hourly\"+j])\n",
    "                    \n",
    "        for j in daily:\n",
    "            if \"daily\"+j in list(visited.keys()) and visited[\"daily\"+j]>=i:\n",
    "                rmse.append(visited[\"daily\"+j])\n",
    "            else:\n",
    "                f=list(featureVector(\"/home/codit/Documents/Time Series JSON/daily/\"+j).values())\n",
    "                if forcastability_score_cosine_with_rep(f)>=i:\n",
    "                    visited[\"daily\"+j]=predict(pd.read_json(\"/home/codit/Documents/Time Series JSON/daily/\"+j))\n",
    "                    rmse.append(visited[\"daily\"+j])\n",
    "        for j in weekly:\n",
    "            if \"weekly\"+j in list(visited.keys()) and visited[\"weekly\"+j]>=i:\n",
    "                rmse.append(visited[\"weekly\"+j])\n",
    "            else:\n",
    "                f=list(featureVector(\"/home/codit/Documents/Time Series JSON/weekly/\"+j).values())\n",
    "                if forcastability_score_cosine_with_rep(f)>=i:\n",
    "                    visited[\"weekly\"+j]=predict(pd.read_json(\"/home/codit/Documents/Time Series JSON/weekly/\"+j))\n",
    "                    rmse.append(visited[\"weekly\"+j])\n",
    "                    \n",
    "        for j in monthly:\n",
    "            if \"monthly\"+j in list(visited.keys()) and visited[\"monthly\"+j]>=i:\n",
    "                rmse.append(visited[\"monthly\"+j])\n",
    "            else:\n",
    "                f=list(featureVector(\"/home/codit/Documents/Time Series JSON/monthly/\"+j).values())\n",
    "                if forcastability_score_cosine_with_rep(f)>=i:\n",
    "                    visited[\"monthly\"+j]=predict(pd.read_json(\"/home/codit/Documents/Time Series JSON/monthly/\"+j))\n",
    "                    rmse.append(visited[\"monthly\"+j])\n",
    "                    \n",
    "        \n",
    "        if len(rmse)!=0:\n",
    "            mean=sum(rmse)/len(rmse)\n",
    "            print(mean)\n",
    "            if minMAPEGen>mean:\n",
    "                minMAPEGen=mean\n",
    "                threshold=i\n",
    "                \n",
    "    return threshold\n",
    "                \n",
    "    \n",
    "def predict(data):\n",
    "    \n",
    "    data=data.rename(columns={'point_timestamp':'ds','point_value':'y'})\n",
    "    data['ds'] = pd.to_datetime(data['ds'])\n",
    "    data['ds'] = data['ds'].dt.tz_localize(None)\n",
    "    model=Prophet()\n",
    "    #have 80 percent of the data for training\n",
    "    train = data.iloc[:int(0.8*len(data))]\n",
    "    train.replace(0, train['y'].mean(), inplace=True)\n",
    "    test = data.iloc[int(0.8*len(data)):]\n",
    "    test.replace(0,train['y'].mean(),inplace=True)\n",
    "    model.fit(train)\n",
    "    forecast=model.predict(test)\n",
    "    return root_mean_squared_error(test['y'],forecast['yhat'].tail(len(test)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T10:56:05.580199Z",
     "start_time": "2024-03-27T10:56:01.370385Z"
    }
   },
   "id": "1d28cdf3bcdff6c7",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "result=forecastabilityThreshold(sampleforecastScores)\n",
    "threshold = {\"threshold\":result}\n",
    "with open('threshold.json', 'w') as f:\n",
    "    json.dump(threshold, f)\n",
    "    \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ec8e131a36a1b4f",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
